{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8536c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3178a",
   "metadata": {},
   "source": [
    "# Congressional District Population (Nationwide) + Districts using Adjusted Data Population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21730edd",
   "metadata": {},
   "source": [
    "## 2022 Congressional Districts with Total Population from Census PL file 10/18/2022\n",
    "\n",
    "### Background:\n",
    "We received a data request asking for total populations of the 2022 congressional districts.\n",
    "\n",
    "Note that the following states use adjusted census data when drawing congressional districts: CA, MD, NJ, NV, RI, VA, WA. For the purposes of calculating population deviations, the adjusted population totals (calculated below) should be used.\n",
    "\n",
    "### Approach:\n",
    "\n",
    "- Load National BAF file, which contains a 2020 Census PL Total Population field\n",
    "- Groupby congressional district, and join to the national 2022 congressional file\n",
    "- Check file  \n",
    "- Export file\n",
    "\n",
    "### Links to Download Raw Files \n",
    "- [National BAF for 2022 Districts](https://redistrictingdatahub.org/dataset/national-block-assignment-file-for-2022-state-legislative-and-congressional-districts/)\n",
    "\n",
    "### Processing Steps:\n",
    "See attached notebook\n",
    "\n",
    "**Note: A full \"raw-from-source\" file is also available upon request. Please email info@redistrictingdatahub.org for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1957d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0q4w6hm92mg_zxd84dfxmq3m0000gn/T/ipykernel_61023/3608852637.py:2: DtypeWarning: Columns (3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  updated_baf_boundary = pd.read_csv(\"./raw-from-source/national_baf_boundary/national_baf_boundary.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Load in the new BAF file\n",
    "updated_baf_boundary = pd.read_csv(\"./raw-from-source/national_baf_boundary/national_baf_boundary.csv\")\n",
    "\n",
    "# Create a CD ID\n",
    "updated_baf_boundary[\"CONG-ID\"] = updated_baf_boundary[\"STATE\"]+\"-\"+updated_baf_boundary[\"CONG\"].astype(str)\n",
    "\n",
    "# Groupby this ID\n",
    "cong_totals = updated_baf_boundary.groupby([\"CONG-ID\"]).sum()\n",
    "\n",
    "# Reset index, drop columns, cast pop data to integer and remove \"NO VALUE\" districts\n",
    "cong_totals.reset_index(drop = False, inplace = True)\n",
    "cong_totals.drop([\"GEOID20\"], axis = 1, inplace = True)\n",
    "cong_totals[\"P0010001\"] = cong_totals[\"P0010001\"].astype(int)\n",
    "cong_totals = cong_totals[~cong_totals[\"CONG-ID\"].str.contains(\"NO\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d98aa0",
   "metadata": {},
   "source": [
    "## Clean File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b327b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330759736"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the columns\n",
    "cong_totals[\"STATE\"] = cong_totals[\"CONG-ID\"].apply(lambda x: x.split(\"-\")[0])\n",
    "cong_totals[\"DISTRICT\"] = cong_totals[\"CONG-ID\"].apply(lambda x: \"-\".join(x.split(\"-\")[1:]))\n",
    "\n",
    "# Filter the columns\n",
    "cong_totals = cong_totals[[\"STATE\",\"DISTRICT\",\"CONG-ID\",\"P0010001\"]]\n",
    "\n",
    "# Final population check\n",
    "sum(cong_totals[\"P0010001\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97014d3",
   "metadata": {},
   "source": [
    "## Check Against Prior File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0e0103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [STATE_x, DISTRICT_x, CONG-ID, P0010001_x, STATE_y, DISTRICT_y, CD_ID, P0010001_y, _merge]\n",
      "Index: []\n",
      "0    435\n",
      "Name: Pop_Diff, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load in old file as a check\n",
    "old_cong_file = pd.read_csv(\"./raw-from-source/cd_pop_2022_csv/cd_pop_2022_csv.csv\")\n",
    "\n",
    "# Define a dictionary to make the join to the older file work\n",
    "cong_dict = {\"2801\":\"1\",\n",
    "\"2802\":\"2\",\n",
    "\"2803\":\"3\",\n",
    "\"2804\":\"4\"}\n",
    "\n",
    "# Join the two together\n",
    "old_cong_file[\"DISTRICT\"] = old_cong_file[\"DISTRICT\"].map(cong_dict).fillna(old_cong_file[\"DISTRICT\"])\n",
    "old_cong_file[\"CONG-ID\"] = old_cong_file[\"STATE\"] + \"-\" + old_cong_file[\"DISTRICT\"].astype(str).str.upper()\n",
    "combined = pd.merge(cong_totals, old_cong_file, how = \"outer\", on = \"CONG-ID\", indicator = True)\n",
    "\n",
    "# Confirm that everything joins\n",
    "print(combined[combined[\"_merge\"]!=\"both\"])\n",
    "\n",
    "# See if there are any differences across the two files\n",
    "combined[\"Pop_Diff\"] = combined[\"P0010001_x\"] - combined[\"P0010001_y\"]\n",
    "print(combined[\"Pop_Diff\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6abcc",
   "metadata": {},
   "source": [
    "## Check File On Its Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5cfd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA    52\n",
      "TX    38\n",
      "FL    28\n",
      "NY    26\n",
      "IL    17\n",
      "PA    17\n",
      "OH    15\n",
      "GA    14\n",
      "NC    14\n",
      "MI    13\n",
      "NJ    12\n",
      "VA    11\n",
      "WA    10\n",
      "TN     9\n",
      "IN     9\n",
      "AZ     9\n",
      "MA     9\n",
      "WI     8\n",
      "MO     8\n",
      "MN     8\n",
      "CO     8\n",
      "MD     8\n",
      "SC     7\n",
      "AL     7\n",
      "OR     6\n",
      "LA     6\n",
      "KY     6\n",
      "OK     5\n",
      "CT     5\n",
      "IA     4\n",
      "NV     4\n",
      "KS     4\n",
      "MS     4\n",
      "UT     4\n",
      "AR     4\n",
      "NM     3\n",
      "NE     3\n",
      "RI     2\n",
      "ME     2\n",
      "WV     2\n",
      "HI     2\n",
      "ID     2\n",
      "NH     2\n",
      "MT     2\n",
      "VT     1\n",
      "AK     1\n",
      "SD     1\n",
      "ND     1\n",
      "DE     1\n",
      "WY     1\n",
      "Name: STATE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Can compare these numbers against this link: https://www.census.gov/library/visualizations/2021/dec/2020-apportionment-map.html\n",
    "print(cong_totals[\"STATE\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a04f1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330759736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for 330759736 (population of all 50 states)\n",
    "sum(cong_totals[\"P0010001\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eabffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR\n",
      "MAX: 753219\n",
      "MIN: 752509\n",
      "\n",
      "CA\n",
      "MAX: 782247\n",
      "MIN: 754875\n",
      "\n",
      "CO\n",
      "MAX: 721794\n",
      "MIN: 721664\n",
      "\n",
      "HI\n",
      "MAX: 728876\n",
      "MIN: 726395\n",
      "\n",
      "IA\n",
      "MAX: 797645\n",
      "MIN: 797551\n",
      "\n",
      "LA\n",
      "MAX: 776333\n",
      "MIN: 776268\n",
      "\n",
      "MD\n",
      "MAX: 777845\n",
      "MIN: 767247\n",
      "\n",
      "MI\n",
      "MAX: 775666\n",
      "MIN: 774544\n",
      "\n",
      "NE\n",
      "MAX: 653847\n",
      "MIN: 653822\n",
      "\n",
      "NJ\n",
      "MAX: 779056\n",
      "MIN: 771744\n",
      "\n",
      "NM\n",
      "MAX: 705846\n",
      "MIN: 705832\n",
      "\n",
      "NV\n",
      "MAX: 778140\n",
      "MIN: 773758\n",
      "\n",
      "RI\n",
      "MAX: 549301\n",
      "MIN: 548078\n",
      "\n",
      "VA\n",
      "MAX: 788614\n",
      "MIN: 779587\n",
      "\n",
      "WA\n",
      "MAX: 774871\n",
      "MIN: 768710\n",
      "\n",
      "WV\n",
      "MAX: 897649\n",
      "MIN: 896067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check States with large differences\n",
    "# Some of these states didn't exactly follow +/- 1 deviation or used adjusted data\n",
    "def check_max_min(joined_cong):\n",
    "    for val in list(joined_cong[\"STATE\"].unique()):\n",
    "        '''print(val)\n",
    "        print(\"MAX:\", max(joined_cong[joined_cong[\"STATE\"]==val][\"P0010001\"]))\n",
    "        print(\"MIN:\", min(joined_cong[joined_cong[\"STATE\"]==val][\"P0010001\"]))\n",
    "        print(\"\")'''\n",
    "        #print(\"REPORT\")\n",
    "        if abs(max(joined_cong[joined_cong[\"STATE\"]==val][\"P0010001\"]) - min(joined_cong[joined_cong[\"STATE\"]==val][\"P0010001\"]))>10:\n",
    "            print(val)\n",
    "            print(\"MAX:\", max(joined_cong[joined_cong[\"STATE\"]==val][\"P0010001\"]))\n",
    "            print(\"MIN:\", min(joined_cong[joined_cong[\"STATE\"]==val][\"P0010001\"]))\n",
    "            print(\"\")\n",
    "\n",
    "check_max_min(cong_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c67f36",
   "metadata": {},
   "source": [
    "## Export File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195362d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "cong_totals.to_csv(\"./cong_totals.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8bf2f",
   "metadata": {},
   "source": [
    "## 2022 Districts using Adjusted Data - Populations  10/18/2022 \n",
    "\n",
    "### Background:\n",
    "- We received a data request asking for total populations of the 2022 districts.\n",
    "- Although most states draw their redistricting plans using the census' population, a handful of states use adjusted data.\n",
    "- The usage of adjusted data is made more complicated by the fact that not every state that produces adjusted data uses it for all levels of redistricting.\n",
    "- Below is a list of the states that produce adjusted data and the level(s) of redistricting they use it for:\n",
    "    - CA (Congressional and State Legislative)\n",
    "    - CO (State Legislative)\n",
    "    - CT (State Legislative)\n",
    "    - DE (State Legislative)\n",
    "    - HI (State Legislative)\n",
    "    - MD (Congressional and State Legislative)\n",
    "    - MT (State Legislative)\n",
    "    - NJ (Congressional and State Legislative)\n",
    "    - NV (Congressional and State Legislative)\n",
    "    - NY (State Legislative)\n",
    "    - PA (State Legislative)\n",
    "    - RI (Congressional and State Legislative)\n",
    "    - VA (Congressional and State Legislative)\n",
    "    - WA (Congressional and State Legislative)\n",
    "- Due to this nuance, we thought it would make sense to produce a dataset with the districts that used adjusted data and their adjusted population, rather than adding in an \"adjusted population\" column to the national block-assignment file, where the adjusted population would not be relevant in all cases.\n",
    "- Furthermore, RI did not release block-level adjusted data, but they did release their district-level adjusted populations.\n",
    "\n",
    "### Approach:\n",
    "- For every state on the above list, except RI, load in files containing adjusted populations for each block.\n",
    "  - Note, these files were produced for earlier work and involve manipulating states' adjusted datasets\n",
    "- Join the adjusted block-level populations to the national block assignment file.\n",
    "- Transform the block assignment file so that every row is now a particular congressional, state house, or state senate district with its population\n",
    "- For RI, transcribe the district populations from an official report and join these populations to the relevant districts.\n",
    "- Check the file\n",
    "- Export the file   \n",
    "\n",
    "### Links to Download Raw Files\n",
    "- RI District Population Reports\n",
    "  - Official state reports, available upon requests.   \n",
    "- RI District Population csv\n",
    "  - Created by the RDH using the reports, available upon request. \n",
    "- State Block-Level Adjusted Populations for all States except RI (where data not available)\n",
    "  - Produced using official files on the RDH website, processed files available upon request.   \n",
    "- [National BAF for 2022 Districts](https://redistrictingdatahub.org/dataset/national-block-assignment-file-for-2022-state-legislative-and-congressional-districts/)\n",
    "    \n",
    "### Processing Steps\n",
    "- See attached notebook\n",
    "\n",
    "#### Note: A full \"raw-from-source\" file is also available upon request. Please email info@redistrictingdatahub.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "568eba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the national BAF\n",
    "national_baf = pd.read_csv(\"./raw-from-source/national_baf_boundary/national_baf_boundary.csv\", dtype =({\"GEOID20\":str, \"STATEAB\":str, \"CONG\":str, \"SLDU\":str, \"SLDL\":str, \"FLOTERIAL\":str}))\n",
    "\n",
    "# Create columns for the various districts\n",
    "national_baf[\"UNQ_CONG_DIST_ID\"] = national_baf[\"STATE\"] + \"-\" + national_baf[\"CONG\"].astype(str)\n",
    "national_baf[\"UNQ_SLDL_DIST_ID\"] = national_baf[\"STATE\"] + \"-\" + national_baf[\"SLDL\"].astype(str)\n",
    "national_baf[\"UNQ_SLDU_DIST_ID\"] = national_baf[\"STATE\"] + \"-\" + national_baf[\"SLDU\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "514a762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the GEOID column\n",
    "national_baf[\"GEOID20\"] = national_baf[\"GEOID20\"].astype(str).str.zfill(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5237fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_data_state_subset = ['CA', 'CO', 'CT', 'DE', 'HI', 'MD', 'MT', 'NJ', 'NV',\n",
    "'NY', 'PA', 'VA', 'WA']\n",
    "\n",
    "def mod_census(block_id):\n",
    "    block_id = str(block_id)\n",
    "    \n",
    "    # PA appends a letter to the GEOID for the split blocks\n",
    "    if \"A\" in block_id or \"B\" in block_id or \"C\" in block_id:\n",
    "        \n",
    "        # Return the GEOID with out the split so the blocks can be combined\n",
    "        return block_id[:len(block_id)-1]\n",
    "    \n",
    "    # If it's not one of these special blocks, just return the GEOID\n",
    "    else:\n",
    "        return block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb779ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0q4w6hm92mg_zxd84dfxmq3m0000gn/T/ipykernel_61023/2317350772.py:8: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  adj_state = pd.read_csv(\"./raw-from-source/Adjusted_Counts/\"+state+\"_blocks.csv\")\n",
      "/var/folders/1t/0q4w6hm92mg_zxd84dfxmq3m0000gn/T/ipykernel_61023/2317350772.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  adj_state[\"mod_GEOID20\"] = adj_state[\"GEOID20\"].apply(lambda x: mod_census(x))\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the state data\n",
    "adjusted_data_list = []\n",
    "\n",
    "# Iterate over the states\n",
    "for state in adjusted_data_state_subset:\n",
    "    \n",
    "    # Load and filter the data\n",
    "    adj_state = pd.read_csv(\"./raw-from-source/Adjusted_Counts/\"+state+\"_blocks.csv\")\n",
    "    adj_state = adj_state[[\"GEOID20\", \"Adj_Pop\"]]\n",
    "    \n",
    "    # Deal with PA split blocks\n",
    "    if state == \"PA\":\n",
    "        \n",
    "        # Use the above function to return the \"unsplit\" GEOID\n",
    "        adj_state[\"mod_GEOID20\"] = adj_state[\"GEOID20\"].apply(lambda x: mod_census(x))\n",
    "        \n",
    "        # Because the splits blocks are in the same districts, we can join them together to match PL geographies\n",
    "        adj_state_mod = adj_state.groupby(\"mod_GEOID20\").sum()\n",
    "        \n",
    "        # Clean the index and rename columns to match others\n",
    "        adj_state_mod.reset_index(drop = False, inplace = True)\n",
    "        adj_state_mod.rename(columns = {\"mod_GEOID20\":\"GEOID20\"}, inplace = True)\n",
    "        adj_state_mod = adj_state_mod[[\"GEOID20\", \"Adj_Pop\"]]\n",
    "        \n",
    "        # Append the PA data to the list\n",
    "        adjusted_data_list.append(adj_state_mod)\n",
    "    \n",
    "    # For other states just add the data\n",
    "    else:\n",
    "        adjusted_data_list.append(adj_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebcec008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the list to the dataframe\n",
    "adj_state_data_df = pd.concat(adjusted_data_list)\n",
    "\n",
    "# Clean the columns\n",
    "adj_state_data_df[\"Adj_Pop\"] = adj_state_data_df[\"Adj_Pop\"].astype(int)\n",
    "adj_state_data_df[\"GEOID20\"] = adj_state_data_df[\"GEOID20\"].astype(str).str.zfill(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd8f1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left_only     6067019\n",
       "both          2059937\n",
       "right_only          0\n",
       "Name: _merge, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the two files together\n",
    "adjusted_counts = pd.merge(national_baf, adj_state_data_df, how = \"outer\", on = \"GEOID20\", indicator = True)\n",
    "\n",
    "# Check the join\n",
    "adjusted_counts[\"_merge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3053e34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that there is no population for any of the unjoined blocks\n",
    "sum(adjusted_counts[adjusted_counts[\"_merge\"]==\"right_only\"][\"Adj_Pop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8196d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0q4w6hm92mg_zxd84dfxmq3m0000gn/T/ipykernel_61023/3408356727.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  joined[\"Adj_Pop\"] = joined[\"Adj_Pop\"].fillna(0)\n",
      "/var/folders/1t/0q4w6hm92mg_zxd84dfxmq3m0000gn/T/ipykernel_61023/3408356727.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  joined[\"Adj_Pop\"] = joined[\"Adj_Pop\"].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Filter down to join blocks or blocks in RI (need that to get the district data for the state)\n",
    "joined = adjusted_counts[(adjusted_counts[\"_merge\"]==\"both\") | (adjusted_counts[\"STATE\"]==\"RI\")]\n",
    "\n",
    "# Clean the columns\n",
    "joined[\"Adj_Pop\"] = joined[\"Adj_Pop\"].fillna(0)\n",
    "joined[\"Adj_Pop\"] = joined[\"Adj_Pop\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "227946aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(joined[\"STATE\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8951a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MD', 'RI', 'VA', 'NJ', 'CA', 'CT', 'PA', 'WA', 'NY', 'NV', 'HI',\n",
       "       'DE', 'CO', 'MT'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined[\"STATE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba4ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of states that use adjusted data for congressional redistricting\n",
    "uses_cong = joined[joined[\"STATE\"].isin([\"CA\", \"MD\", \"NJ\", \"NV\", \"RI\", \"VA\", \"WA\"])]\n",
    "\n",
    "# Aggregate to the appropriate district levels\n",
    "joined_cong = uses_cong.groupby(\"UNQ_CONG_DIST_ID\").sum()\n",
    "joined_sldl = joined.groupby(\"UNQ_SLDL_DIST_ID\").sum()\n",
    "joined_sldu = joined.groupby(\"UNQ_SLDU_DIST_ID\").sum()\n",
    "\n",
    "# Clean the aggregations\n",
    "joined_cong.reset_index(inplace = True, drop = False)\n",
    "joined_sldl.reset_index(inplace = True, drop = False)\n",
    "joined_sldu.reset_index(inplace = True, drop = False)\n",
    "\n",
    "joined_cong.drop([\"P0010001\"], axis = 1, inplace = True)\n",
    "joined_sldl.drop([\"P0010001\"], axis = 1, inplace = True)\n",
    "joined_sldu.drop([\"P0010001\"], axis = 1, inplace = True)\n",
    "\n",
    "joined_cong.columns = [\"ID\", \"Adj_Pop\"]\n",
    "joined_sldl.columns = [\"ID\", \"Adj_Pop\"]\n",
    "joined_sldu.columns = [\"ID\", \"Adj_Pop\"]\n",
    "\n",
    "joined_cong[\"Level\"] = \"CONG\"\n",
    "joined_sldl[\"Level\"] = \"SLDL\"\n",
    "joined_sldu[\"Level\"] = \"SLDU\"\n",
    "\n",
    "# Join them back into one file\n",
    "combined_files = pd.concat([joined_cong, joined_sldl, joined_sldu])\n",
    "\n",
    "# Get the state abbreviation\n",
    "combined_files[\"State\"] = combined_files[\"ID\"].apply(lambda x: x[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4b60eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ID of the level and the ID so we can join to RI\n",
    "combined_files[\"unique_id\"] = combined_files[\"Level\"]+\"-\"+combined_files[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5df15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the RI data\n",
    "ri_data = pd.read_csv(\"./raw-from-source/ri_sizes.csv\",dtype={\"Number\":str, \"Adj_Pop\":int, \"Level\":str})\n",
    "\n",
    "# Create a unique ID to join with the pop. file\n",
    "ri_data[\"unique_id\"] = ri_data[\"Level\"]+\"-RI-\"+ri_data[\"Number\"].astype(str)\n",
    "\n",
    "# Make the population an integer\n",
    "ri_data[\"Adj_Pop\"] = ri_data[\"Adj_Pop\"].astype(int)\n",
    "\n",
    "# Create a dictionary mapping from district ID to population in RI\n",
    "ri_data_dict = dict(zip(ri_data[\"unique_id\"], ri_data[\"Adj_Pop\"]))\n",
    "\n",
    "# Apply the above dictionary to the RI districts in the combined file\n",
    "combined_files[\"Adj_Pop\"] = combined_files[\"unique_id\"].map(ri_data_dict).fillna(combined_files[\"Adj_Pop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f537ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_files[\"state-level\"] = combined_files[\"State\"]+\"-\"+combined_files[\"Level\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5339b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_files[\"Adj_Pop\"] = combined_files[\"Adj_Pop\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ffc914b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Adj_Pop</th>\n",
       "      <th>Level</th>\n",
       "      <th>State</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>state-level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>CT-NO VALUE</td>\n",
       "      <td>0</td>\n",
       "      <td>SLDL</td>\n",
       "      <td>CT</td>\n",
       "      <td>SLDL-CT-NO VALUE</td>\n",
       "      <td>CT-SLDL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>CT-NO VALUE</td>\n",
       "      <td>0</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>CT</td>\n",
       "      <td>SLDU-CT-NO VALUE</td>\n",
       "      <td>CT-SLDU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>RI-NO VALUE</td>\n",
       "      <td>0</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>RI</td>\n",
       "      <td>SLDU-RI-NO VALUE</td>\n",
       "      <td>RI-SLDU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Adj_Pop Level State         unique_id state-level\n",
       "296  CT-NO VALUE        0  SLDL    CT  SLDL-CT-NO VALUE     CT-SLDL\n",
       "111  CT-NO VALUE        0  SLDU    CT  SLDU-CT-NO VALUE     CT-SLDU\n",
       "467  RI-NO VALUE        0  SLDU    RI  SLDU-RI-NO VALUE     RI-SLDU"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_files[combined_files[\"Adj_Pop\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e78155a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"No Data\" districts (these were unassigned blocks we kept in the BAF)\n",
    "combined_files = combined_files[~combined_files[\"ID\"].str.contains(\"NO\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97412a",
   "metadata": {},
   "source": [
    "## Check On Its Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6edff520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PA-SLDL    203\n",
       "CT-SLDL    151\n",
       "NY-SLDL    150\n",
       "VA-SLDL    100\n",
       "MT-SLDL    100\n",
       "CA-SLDL     80\n",
       "RI-SLDL     75\n",
       "MD-SLDL     71\n",
       "CO-SLDL     65\n",
       "NY-SLDU     63\n",
       "CA-CONG     52\n",
       "HI-SLDL     51\n",
       "PA-SLDU     50\n",
       "MT-SLDU     50\n",
       "WA-SLDL     49\n",
       "WA-SLDU     49\n",
       "MD-SLDU     47\n",
       "NV-SLDL     42\n",
       "DE-SLDL     41\n",
       "VA-SLDU     40\n",
       "CA-SLDU     40\n",
       "NJ-SLDU     40\n",
       "NJ-SLDL     40\n",
       "RI-SLDU     38\n",
       "CT-SLDU     36\n",
       "CO-SLDU     35\n",
       "HI-SLDU     25\n",
       "DE-SLDU     21\n",
       "NV-SLDU     21\n",
       "NJ-CONG     12\n",
       "VA-CONG     11\n",
       "WA-CONG     10\n",
       "MD-CONG      8\n",
       "NV-CONG      4\n",
       "RI-CONG      2\n",
       "Name: state-level, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many of each district type there is (correct numbers are below)\n",
    "combined_files[\"state-level\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163543d9",
   "metadata": {},
   "source": [
    "Target District Numbers\n",
    "\n",
    "- PA-SLDL    203\n",
    "- CT-SLDL    151\n",
    "- NY-SLDL    150\n",
    "- VA-SLDL    100\n",
    "- MT-SLDL    100\n",
    "- CA-SLDL     80\n",
    "- RI-SLDL     75\n",
    "- MD-SLDL     71\n",
    "- CO-SLDL     65\n",
    "- NY-SLDU     63\n",
    "- CA-CONG     52\n",
    "- HI-SLDL     51\n",
    "- PA-SLDU     50\n",
    "- MT-SLDU     50\n",
    "- WA-SLDL     49\n",
    "- WA-SLDU     49\n",
    "- MD-SLDU     47\n",
    "- NV-SLDL     42\n",
    "- DE-SLDL     41\n",
    "- VA-SLDU     40\n",
    "- CA-SLDU     40\n",
    "- NJ-SLDU     40\n",
    "- NJ-SLDL     40\n",
    "- RI-SLDU     38\n",
    "- CT-SLDU     36\n",
    "- CO-SLDU     35\n",
    "- HI-SLDU     25\n",
    "- DE-SLDU     21\n",
    "- NV-SLDU     21\n",
    "- NJ-CONG     12\n",
    "- VA-CONG     11\n",
    "- WA-CONG     10\n",
    "- MD-CONG      8\n",
    "- NV-CONG      4\n",
    "- RI-CONG      2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbdc41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-level\n",
      "CA-CONG    39523437\n",
      "CA-SLDL    39523437\n",
      "CA-SLDU    39523437\n",
      "CO-SLDL     5773714\n",
      "CO-SLDU     5773714\n",
      "CT-SLDL     3603566\n",
      "CT-SLDU     3603566\n",
      "DE-SLDL      989598\n",
      "DE-SLDU      989598\n",
      "HI-SLDL     1383606\n",
      "HI-SLDU     1383606\n",
      "MD-CONG     6175403\n",
      "MD-SLDL     6175403\n",
      "MD-SLDU     6175403\n",
      "MT-SLDL     1082717\n",
      "MT-SLDU     1082717\n",
      "NJ-CONG     9283016\n",
      "NJ-SLDL     9283016\n",
      "NJ-SLDU     9283016\n",
      "NV-CONG     3104614\n",
      "NV-SLDL     3104614\n",
      "NV-SLDU     3104614\n",
      "NY-SLDL    20193858\n",
      "NY-SLDU    20193858\n",
      "PA-SLDL    13002700\n",
      "PA-SLDU    13002700\n",
      "RI-CONG     1097379\n",
      "RI-SLDL     1097379\n",
      "RI-SLDU     1097379\n",
      "VA-CONG     8631393\n",
      "VA-SLDL     8631393\n",
      "VA-SLDU     8631393\n",
      "WA-CONG     7705281\n",
      "WA-SLDL     7705281\n",
      "WA-SLDU     7705281\n",
      "Name: Adj_Pop, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the population totals for the various district types\n",
    "state_sums = combined_files.groupby(\"state-level\").sum()\n",
    "print(state_sums[\"Adj_Pop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de695dd8",
   "metadata": {},
   "source": [
    "Target Pops\n",
    "\n",
    "- CA 39523437\n",
    "- CO     5773714\n",
    "- CT     3603566\n",
    "- DE      989598\n",
    "- HI    1383606\n",
    "- MD    6175403\n",
    "- MT    1082717\n",
    "- NJ    9283016\n",
    "- NV     3104614\n",
    "- NY   20193858\n",
    "- PA   13002700\n",
    "- RI    1097379\n",
    "- VA    8631393\n",
    "- WA    7705281"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de054b",
   "metadata": {},
   "source": [
    "## Final File Cleaning and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702c337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_files.drop([\"unique_id\",\"state-level\"], axis = 1, inplace = True)\n",
    "combined_files[\"ID\"] = combined_files[\"ID\"].apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "combined_files_join = combined_files.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "660c62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Cleaning\n",
    "combined_files.columns = [\"DISTRICT\", \"ADJ_POP\", \"LEVEL\", \"STATE\"]\n",
    "combined_files = combined_files[[\"STATE\", \"DISTRICT\", \"LEVEL\", \"ADJ_POP\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45aa3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_files = combined_files.sort_values([\"STATE\", \"LEVEL\", \"DISTRICT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8b8d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_files.to_csv(\"./adjusted_districts_pop.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9645d",
   "metadata": {},
   "source": [
    "## Check against old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d11013bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_adjusted_pops = pd.read_csv(\"./raw-from-source/national_districts_adjusted_pop/adjusted_districts_pop.csv\")\n",
    "\n",
    "combined_files_join[\"DIST-ID\"] = combined_files_join[\"State\"] +\"-\"+combined_files_join[\"Level\"] +\"-\"+  combined_files_join[\"ID\"].astype(str).str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f6b046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_adjusted_pops[\"DIST-ID\"] = old_adjusted_pops[\"State\"]+ \"-\" + old_adjusted_pops[\"Level\"] + \"-\" + old_adjusted_pops[\"ID\"].astype(str).astype(str).str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b445ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_adjusted = pd.merge(combined_files_join, old_adjusted_pops, how = \"outer\", on = \"DIST-ID\", indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfd130fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "both          1872\n",
       "left_only        0\n",
       "right_only       0\n",
       "Name: _merge, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_adjusted[\"_merge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38ffd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_adjusted[\"Difference\"] = joined_adjusted[\"Adj_Pop_x\"] - joined_adjusted[\"Adj_Pop_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d907e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_x</th>\n",
       "      <th>Adj_Pop_x</th>\n",
       "      <th>Level_x</th>\n",
       "      <th>State_x</th>\n",
       "      <th>DIST-ID</th>\n",
       "      <th>ID_y</th>\n",
       "      <th>Adj_Pop_y</th>\n",
       "      <th>Level_y</th>\n",
       "      <th>State_y</th>\n",
       "      <th>_merge</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>30</td>\n",
       "      <td>24889</td>\n",
       "      <td>SLDL</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDL-030</td>\n",
       "      <td>30</td>\n",
       "      <td>24882</td>\n",
       "      <td>SLDL</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>33</td>\n",
       "      <td>23821</td>\n",
       "      <td>SLDL</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDL-033</td>\n",
       "      <td>33</td>\n",
       "      <td>23828</td>\n",
       "      <td>SLDL</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>10</td>\n",
       "      <td>47281</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-010</td>\n",
       "      <td>10</td>\n",
       "      <td>47345</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>11</td>\n",
       "      <td>48081</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-011</td>\n",
       "      <td>11</td>\n",
       "      <td>47647</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>14</td>\n",
       "      <td>49253</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-014</td>\n",
       "      <td>14</td>\n",
       "      <td>49189</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>15</td>\n",
       "      <td>47104</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-015</td>\n",
       "      <td>15</td>\n",
       "      <td>47131</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>17</td>\n",
       "      <td>49042</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-017</td>\n",
       "      <td>17</td>\n",
       "      <td>49015</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>18</td>\n",
       "      <td>48592</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-018</td>\n",
       "      <td>18</td>\n",
       "      <td>48541</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>19</td>\n",
       "      <td>48726</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-019</td>\n",
       "      <td>19</td>\n",
       "      <td>49105</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>20</td>\n",
       "      <td>49198</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-020</td>\n",
       "      <td>20</td>\n",
       "      <td>48961</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>21</td>\n",
       "      <td>49233</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-021</td>\n",
       "      <td>21</td>\n",
       "      <td>49173</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>3</td>\n",
       "      <td>44811</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-003</td>\n",
       "      <td>03</td>\n",
       "      <td>44981</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>4</td>\n",
       "      <td>44999</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-004</td>\n",
       "      <td>04</td>\n",
       "      <td>45012</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>6</td>\n",
       "      <td>48512</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-006</td>\n",
       "      <td>06</td>\n",
       "      <td>48481</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>7</td>\n",
       "      <td>45401</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-007</td>\n",
       "      <td>07</td>\n",
       "      <td>45226</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>9</td>\n",
       "      <td>45402</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE-SLDU-009</td>\n",
       "      <td>09</td>\n",
       "      <td>45828</td>\n",
       "      <td>SLDU</td>\n",
       "      <td>DE</td>\n",
       "      <td>both</td>\n",
       "      <td>-426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_x  Adj_Pop_x Level_x State_x      DIST-ID ID_y  Adj_Pop_y Level_y  \\\n",
       "418    30      24889    SLDL      DE  DE-SLDL-030   30      24882    SLDL   \n",
       "421    33      23821    SLDL      DE  DE-SLDL-033   33      23828    SLDL   \n",
       "1429   10      47281    SLDU      DE  DE-SLDU-010   10      47345    SLDU   \n",
       "1430   11      48081    SLDU      DE  DE-SLDU-011   11      47647    SLDU   \n",
       "1433   14      49253    SLDU      DE  DE-SLDU-014   14      49189    SLDU   \n",
       "1434   15      47104    SLDU      DE  DE-SLDU-015   15      47131    SLDU   \n",
       "1436   17      49042    SLDU      DE  DE-SLDU-017   17      49015    SLDU   \n",
       "1437   18      48592    SLDU      DE  DE-SLDU-018   18      48541    SLDU   \n",
       "1438   19      48726    SLDU      DE  DE-SLDU-019   19      49105    SLDU   \n",
       "1440   20      49198    SLDU      DE  DE-SLDU-020   20      48961    SLDU   \n",
       "1441   21      49233    SLDU      DE  DE-SLDU-021   21      49173    SLDU   \n",
       "1442    3      44811    SLDU      DE  DE-SLDU-003   03      44981    SLDU   \n",
       "1443    4      44999    SLDU      DE  DE-SLDU-004   04      45012    SLDU   \n",
       "1445    6      48512    SLDU      DE  DE-SLDU-006   06      48481    SLDU   \n",
       "1446    7      45401    SLDU      DE  DE-SLDU-007   07      45226    SLDU   \n",
       "1448    9      45402    SLDU      DE  DE-SLDU-009   09      45828    SLDU   \n",
       "\n",
       "     State_y _merge  Difference  \n",
       "418       DE   both           7  \n",
       "421       DE   both          -7  \n",
       "1429      DE   both         -64  \n",
       "1430      DE   both         434  \n",
       "1433      DE   both          64  \n",
       "1434      DE   both         -27  \n",
       "1436      DE   both          27  \n",
       "1437      DE   both          51  \n",
       "1438      DE   both        -379  \n",
       "1440      DE   both         237  \n",
       "1441      DE   both          60  \n",
       "1442      DE   both        -170  \n",
       "1443      DE   both         -13  \n",
       "1445      DE   both          31  \n",
       "1446      DE   both         175  \n",
       "1448      DE   both        -426  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_adjusted[joined_adjusted[\"Difference\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e3eac09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(joined_adjusted[joined_adjusted[\"Difference\"]!=0][\"Difference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "799f04f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIST-ID</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>DE-SLDL-030</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>DE-SLDL-033</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>DE-SLDU-010</td>\n",
       "      <td>-64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>DE-SLDU-011</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>DE-SLDU-014</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>DE-SLDU-015</td>\n",
       "      <td>-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>DE-SLDU-017</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>DE-SLDU-018</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>DE-SLDU-019</td>\n",
       "      <td>-379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>DE-SLDU-020</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>DE-SLDU-021</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>DE-SLDU-003</td>\n",
       "      <td>-170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>DE-SLDU-004</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>DE-SLDU-006</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>DE-SLDU-007</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>DE-SLDU-009</td>\n",
       "      <td>-426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DIST-ID  Difference\n",
       "418   DE-SLDL-030           7\n",
       "421   DE-SLDL-033          -7\n",
       "1429  DE-SLDU-010         -64\n",
       "1430  DE-SLDU-011         434\n",
       "1433  DE-SLDU-014          64\n",
       "1434  DE-SLDU-015         -27\n",
       "1436  DE-SLDU-017          27\n",
       "1437  DE-SLDU-018          51\n",
       "1438  DE-SLDU-019        -379\n",
       "1440  DE-SLDU-020         237\n",
       "1441  DE-SLDU-021          60\n",
       "1442  DE-SLDU-003        -170\n",
       "1443  DE-SLDU-004         -13\n",
       "1445  DE-SLDU-006          31\n",
       "1446  DE-SLDU-007         175\n",
       "1448  DE-SLDU-009        -426"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_adjusted[joined_adjusted[\"Difference\"]!=0][[\"DIST-ID\",\"Difference\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
